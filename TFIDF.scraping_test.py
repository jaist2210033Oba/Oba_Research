import os
import time
import re
import requests
from bs4 import BeautifulSoup
import MeCab
from collections import Counter
from googlesearch import search
from sklearn.feature_extraction.text import TfidfVectorizer

# ä¸è¦èªå¥ã®ãƒªã‚¹ãƒˆ
STOPWORDS = {'ã“ã¨', 'ã‚‚ã®', 'ã“ã‚Œ', 'ãã‚Œ', 'ã¨ã“ã‚', 'ã‚ˆã†', 'ãŸã‚', 'ã»ã‹', 'æ–¹', 'æ™‚', 'äºº',
             'å ´åˆ', 'å¾Œ', 'ä¸­', 'å‰', 'ä»Šæ—¥', 'æ˜¨æ—¥', 'ä»Š', 'èª°', 'ä½•', 'ãªã©', 'ã•ã‚“', 'ä¸€ã¤', 'ã•ã¾ã–ã¾', 'ç§ãŸã¡', 'å¤šã', 'PR', 'æ—¥æœ¬', 'å–ã‚Šçµ„ã¿'}
NOISEWORDS = {'ç·¨é›†', 'è¡¨ç¤º', 'è‹±èªç‰ˆ', 'ISBN', 'å‚ç…§', 'ãƒšãƒ¼ã‚¸', 'æ¤œç´¢', 'è¨˜äº‹', 'ãƒ˜ãƒ«ãƒ—',
              'å±¥æ­´', 'å‡ºå…¸', 'ç›®æ¬¡', 'ã‚«ãƒ†ã‚´ãƒª', 'é …ç›®', 'éè¡¨ç¤º', 'ãƒ¡ãƒ‹ãƒ¥ãƒ¼', 'ãƒ„ãƒ¼ãƒ«', 'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰', 'å°åˆ·'}

# URLåé›†
def fetch_top_urls(query, num_results=15):
    urls = []
    for url in search(query, num_results=num_results + 5):
        if not url.lower().endswith('.pdf'):
            urls.append(url)
        if len(urls) >= num_results:
            break
        time.sleep(1)
    return urls

# HTMLãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
def extract_text_from_url(url):
    headers = {'User-Agent': 'Mozilla/5.0'}
    res = requests.get(url, headers=headers, timeout=10)
    res.encoding = 'utf-8'
    res.raise_for_status()
    soup = BeautifulSoup(res.text, 'html.parser')
    texts = soup.find_all(['p', 'div'])
    content = '\n'.join([t.get_text(strip=True) for t in texts if t.get_text(strip=True)])
    return content

# MeCabãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆè¤‡åˆåè©å¯¾å¿œï¼‰
def tokenize_mecab(text):
    tagger = MeCab.Tagger()
    tagger.parse('')
    node = tagger.parseToNode(text)
    words, buffer = [], []

    while node:
        surface = node.surface
        features = node.feature.split(',')

        if features[0] == 'åè©' and features[1] not in ['éè‡ªç«‹', 'ä»£åè©', 'æ•°']:
            buffer.append(surface)
        else:
            if buffer:
                compound = ''.join(buffer)
                if is_valid_token(compound):
                    words.append(compound)
                buffer = []
        node = node.next

    if buffer:
        compound = ''.join(buffer)
        if is_valid_token(compound):
            words.append(compound)

    return words

# æœ‰åŠ¹èªã®æ¡ä»¶ï¼ˆé•·ã•ãƒ»è¨˜å·ãƒ»æ•°å­—ã®ã¿ã‚’é™¤å¤–ï¼‰
def is_valid_token(word):
    if len(word) <= 1 or len(word) > 20:
        return False
    if word in STOPWORDS or word in NOISEWORDS:
        return False
    if re.fullmatch(r'\d+|[a-zA-Z]+|[^\w\s]', word):
        return False
    if re.fullmatch(r'[\W_]+', word):
        return False
    return True

# TF-IDFç”¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶
def tokenizer_for_tfidf(text):
    return tokenize_mecab(text)

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main():
    query = input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
    focus_words_input = input("æ³¨ç›®å˜èªï¼ˆã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šï¼‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
    focus_words = focus_words_input.split()

    urls = fetch_top_urls(query)
    corpus = []
    total_counter = Counter()

    for idx, url in enumerate(urls, 1):
        print(f"\n==== [{idx}] {url} ====")
        try:
            text = extract_text_from_url(url)
            corpus.append(text)
            tokens = tokenize_mecab(text)
            total_counter.update(tokens)
        except Exception as e:
            print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    # é »å‡ºèªãƒ©ãƒ³ã‚­ãƒ³ã‚°
    print("\nğŸ“Š é »å‡ºèªãƒ©ãƒ³ã‚­ãƒ³ã‚° Top 30")
    for word, count in total_counter.most_common(30):
        print(f"{word}: {count}å›")

    # TF-IDFã‚¹ã‚³ã‚¢è¨ˆç®—
    vectorizer = TfidfVectorizer(tokenizer=tokenizer_for_tfidf)
    tfidf_matrix = vectorizer.fit_transform(corpus)
    feature_names = vectorizer.get_feature_names_out()
    mean_scores = tfidf_matrix.mean(axis=0).A1
    word_score_pairs = list(zip(feature_names, mean_scores))
    word_score_pairs.sort(key=lambda x: x[1], reverse=True)

    print("\nğŸ“Š TF-IDF ãƒ©ãƒ³ã‚­ãƒ³ã‚° Top 30")
    for word, score in word_score_pairs[:30]:
        print(f"{word}: {score:.4f}")

    # æ³¨ç›®èªã®è©•ä¾¡ï¼ˆå…¨æ–‡ã«å¯¾ã—ã¦ç›´æ¥ä¸€è‡´æ¤œç´¢ï¼‰
    print("\nğŸ” æ³¨ç›®èªã®è©•ä¾¡")
    for fw in focus_words:
        raw_freq = sum(text.count(fw) for text in corpus)
        tfidf = next((score for word, score in word_score_pairs if word == fw), 0.0)
        print(f"{fw}: å‡ºç¾å›æ•° = {raw_freq}å›, TF-IDF = {tfidf:.4f}")

if __name__ == "__main__":
    main()
